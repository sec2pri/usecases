{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking into the use of retired HGNCs in paper abstracts using the EuropePMC search API\n",
    "\n",
    "## 1. Overview\n",
    "Please write purpose of the analyses\n",
    "\n",
    "\n",
    "## 2. Define query parameters, set up query\n",
    "We use the EuropePMC search API call ([documentation](https://europepmc.org/RestfulWebService#!/Europe32PMC32Articles32RESTful32API/search)) to retrieve all matching publications for each of the identifiers. Because each query is limited to 25 results, we need to define a function that iterates over the page cursors until the whole list of matches is retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query and processing parameters\n",
    "USER_AGENT = \"Mozilla/5.0\"\n",
    "page_size = 1000\n",
    "cursor_mark = '*'\n",
    "format_type = 'json'\n",
    "base_url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query={}&resultType=core&cursorMark={}&pageSize=25&format={}\"\n",
    "relevant_columns = ['pmid', 'pmcid', 'doi', 'title', 'pubYear', 'abstractText'] # set yourself to filter the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_identifier(identifier, base_url=base_url, cursor_mark=cursor_mark, page_size=page_size, format_type=format_type, relevant_columns=relevant_columns):\n",
    "    \"\"\"Searches EuropePMC for the provided identifier, returns a list with all data for matches\"\"\"\n",
    "    # Initialize the list to store retracted articles\n",
    "    matches = []\n",
    "    # Make the initial request to get the total number of results\n",
    "    url = base_url.format(identifier, cursor_mark, format_type)\n",
    "    try:\n",
    "        response = requests.get(url).json()\n",
    "        total = response['hitCount']\n",
    "        # Check if there are any results\n",
    "        if 'resultList' not in response:\n",
    "            print(\"No results found.\")\n",
    "            return pd.DataFrame()\n",
    "        # Calculate the number of requests needed to retrieve all results\n",
    "        matches.extend(response['resultList']['result'])\n",
    "        num_requests = (total + page_size - 1) // page_size\n",
    "        # Iterate through each page and append the results to the list\n",
    "        while response['nextCursorMark'] is not None:\n",
    "            cursor_mark = response['nextCursorMark']\n",
    "            url = base_url.format(identifier, cursor_mark, format_type)\n",
    "            try:\n",
    "                response = requests.get(url).json()\n",
    "                matches.extend(response['resultList']['result'])\n",
    "            except Exception as e:\n",
    "                #print(\"An error occurred: \" + str(e))\n",
    "                break   \n",
    "    except Exception as e:\n",
    "        #print(\"An error occurred: \" + str(e))\n",
    "        pass\n",
    "    # Get the intersection of columns between the DataFrame and relevant_columns\n",
    "    df = pd.DataFrame(matches)\n",
    "    common_columns = list(set(df.columns) & set(relevant_columns))\n",
    "    # Create a new DataFrame with only the common columns\n",
    "    try:\n",
    "        res = df.loc[:, common_columns]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: {e} column not found.\")\n",
    "        res = df\n",
    "    res = res.fillna(\"NA\")\n",
    "    return res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Uses one ifentifier to see how it works ('cbbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(search_identifier('cbbm'))\n",
    "test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply for all HGNCs\n",
    "We have a table of HGNCs in [hgnc.tsv](data/hgnc.tsv). We'll apply `search_identifier` for each identifier, then remove all duplicated matches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc_df = pd.read_csv('data/hgnc.tsv', sep='\\t')\n",
    "hgnc_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to build a data frame with all unique hits. It will take a long time to run, partly because we wait some seconds between ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_search(column, hgnc_df=hgnc_df):\n",
    "    results_df = pd.DataFrame()\n",
    "    total = len(hgnc_df[column])\n",
    "    for i in tqdm(range(len(hgnc_df[column])), total=total, desc=\"Progress\"):\n",
    "        id = hgnc_df[column][i]\n",
    "        print(f\"\\rProcessing {column}: {id} ({i} of {total}) \", end=\"\", flush=True)\n",
    "        primary_id = hgnc_df['primary_hgnc_id'][i]\n",
    "        secondary_id = hgnc_df['secondary_hgnc_id'][i]\n",
    "        if id not in ['NA', 'Entry Withdrawn']:\n",
    "            if column == 'primary_symbol':\n",
    "                other = 'secondary_symbol'\n",
    "            if column == 'secondary_symbol':\n",
    "                other = 'primary_symbol'\n",
    "            other_id = hgnc_df[other][i]            \n",
    "            search = search_identifier(identifier=id)\n",
    "            # Check if id and other_id exist in the abstracts\n",
    "            if 'abstractText' in search.columns:\n",
    "                in_abstract = ['yes' if re.search(re.escape(id), search['abstractText'][i]) else 'no'for i in range(len(search))]\n",
    "                other_in_abstract = ['yes' if re.search(re.escape(other_id), search['abstractText'][i]) else 'no' for i in range(len(search))]\n",
    "            search[column] = [id for i in range(len(search))]\n",
    "            search[other] = [other_id for i in range(len(search))]\n",
    "            search['primary_hgnc_id'] = [primary_id for i in range(len(search))]\n",
    "            search['secondary_hgnc_id'] = [secondary_id for i in range(len(search))]\n",
    "            search['in_abstract'] = in_abstract\n",
    "            search['other_in_abstract'] = other_in_abstract\n",
    "            # Generate a random number between 5 and 15 (inclusive)\n",
    "            random_seconds = random.uniform(5, 15)\n",
    "            # Pause the program for the random number of seconds\n",
    "            time.sleep(random_seconds)\n",
    "            results_df = pd.concat([results_df, search], ignore_index=True)\n",
    "            results_df.drop_duplicates(inplace=True)\n",
    "            results_df['type'] = [column for i in range(len(results_df))]\n",
    "        else:\n",
    "            pass\n",
    "    return results_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Searching for publications matching all identifiers in the table\n",
    "\n",
    "We split the search \n",
    "\n",
    "Construct results table for primary identifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primaries = multiple_search(column='primary_symbol', hgnc_df=hgnc_df)\n",
    "primaries.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosntruct results table for secondary identifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondaries = multiple_search(column='secondary_symbol', hgnc_df=hgnc_df)\n",
    "secondaries.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc_abs = pd.concat(primaries, secondaries)\n",
    "hgnc_abs.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plotly_plot(dataframe):\n",
    "    # Group the dataframe by 'pubYear' and 'primary_symbol' columns and count occurrences\n",
    "    primary_symbol_counts = dataframe.groupby(['pubYear', 'primary_symbol']).size().reset_index(name='count')\n",
    "    # Group the dataframe by 'pubYear' and 'secondary_symbol' columns and count occurrences\n",
    "    secondary_symbol_counts = dataframe.groupby(['pubYear', 'secondary_symbol']).size().reset_index(name='count')\n",
    "    # Concatenate the two dataframes to combine counts for both 'primary_symbol' and 'secondary_symbol'\n",
    "    combined_counts = pd.concat([primary_symbol_counts, secondary_symbol_counts])\n",
    "    # Create the plot using plotly express\n",
    "    fig = px.bar(combined_counts, x='pubYear', y='count', color='primary_symbol',\n",
    "                 labels={'count': 'Occurrences', 'pubYear': 'Publication Year', 'primary_symbol': 'Symbol'},\n",
    "                 title='Occurrences of Categorical Variables Over Time')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the plot\n",
    "plot = generate_plotly_plot(hgnc_abs)\n",
    "\n",
    "# Show the plot\n",
    "plot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgnc_abs.to_csv('results/abs.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this method might lead to false positives -we are not actually confirming that the symbol is in the abstract. Maybe use regex before to filter out the FPs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
